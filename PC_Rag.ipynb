{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import CTransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch 2.1.2+cu118 (cuda)\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.is_available()\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using torch {torch.__version__} ({DEVICE})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Personal\\Experiments\\om\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "bge_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={'device': 'cuda'},\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = [\n",
    "    PyPDFLoader(r\"C:\\\\Users\\\\ishaan.kohli\\\\Downloads\\\\RAFT.pdf\")\n",
    "\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "vectorstore = Chroma(collection_name=\"split_parents\", embedding_function=bge_embeddings,collection_metadata={\"hnsw:space\": \"cosine\"}, persist_directory=\"stores/PC_memory\")\n",
    "# load_vector_store = Chroma(persist_directory=\"stores/pet_cosine\", embedding_function=bge_embeddings)\n",
    "store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_chunks_retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_chunks_retriever.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Initialized...\n"
     ]
    }
   ],
   "source": [
    "local_llm = r\"D://LLM_Models//zephyr-7b-beta.Q5_K_S.gguf\"\n",
    "\n",
    "config = {\n",
    "'max_new_tokens': 1024,\n",
    "'repetition_penalty': 1.1,\n",
    "'temperature': 0.1,\n",
    "'top_k': 50,\n",
    "'top_p': 0.9,\n",
    "'stream': True,\n",
    "'threads': int(os.cpu_count() / 2)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "llm = CTransformers(\n",
    "    model=local_llm,\n",
    "    model_type=\"mistral\",\n",
    "    lib=\"avx2\", #for CPU use\n",
    "    **config\n",
    ")\n",
    "\n",
    "print(\"LLM Initialized...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(text_input):\n",
    "    query = text_input\n",
    "\n",
    "    retrieved_docs = big_chunks_retriever.get_relevant_documents(query)\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    texts = text_splitter.split_documents(retrieved_docs)\n",
    "\n",
    "    vector_store = Chroma.from_documents(texts, bge_embeddings, collection_metadata={\"hnsw:space\": \"cosine\"}, persist_directory=\"stores/relevant_from_PC\")\n",
    "\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=['context', 'question'])\n",
    "    \n",
    "    load_vector_store = Chroma(persist_directory=\"stores\\\\relevant_from_PC\", embedding_function=bge_embeddings)\n",
    "\n",
    "    # load_vector_store = vector_store\n",
    "\n",
    "\n",
    "    retriever = load_vector_store.as_retriever(search_kwargs={\"k\":2})\n",
    "\n",
    "\n",
    "    chain_type_kwargs = {\"prompt\": prompt}\n",
    "\n",
    "    qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True, chain_type_kwargs=chain_type_kwargs, verbose=True)\n",
    "\n",
    "    final_sol = qa(query)\n",
    "    print(final_sol)\n",
    "    \n",
    "    return final_sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Personal\\Experiments\\om\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:189: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "d:\\Personal\\Experiments\\om\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:189: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "d:\\Personal\\Experiments\\om\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:189: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "d:\\Personal\\Experiments\\om\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:189: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'query': 'What are the benifits of RAFT?', 'result': \"RAFT (Reference Attentive Fusion Transformer) is a technique that enables models to learn domain-specific knowledge through fine-tuning while ensuring robustness against inaccurate document retrievals by understanding the dynamics between the question, the retrieved documents, and the appropriate answer. RAFT trains the model to ignore irrelevant or distracting documents and cites verbatim the relevant sequence from the retrieved documents that would help answer the question, coupled with its chain-of-thought-style response to improve the model's ability to reason consistently across domain specific benchmark datasets like RAG.\", 'source_documents': [Document(page_content='performance. RAFT aims to not only enable models to learn\\ndomain specific knowledge through fine-tuning, but also\\nto ensure robustness against inaccurate retrievals. This is\\nachieved by training the models to understand the dynamics\\nbetween the question posed (prompt), the domain specific\\ndocuments retrieved, and the appropriate answer. Going\\nback to our analogy, our approach is analogous to study-\\ning for an open-book exam by recognizing relevant, and\\nirrelevant retrieved documents.', metadata={'page': 1, 'source': 'C:\\\\\\\\Users\\\\\\\\ishaan.kohli\\\\\\\\Downloads\\\\\\\\RAFT.pdf'}), Document(page_content='tion, and a set of retrieved documents, we train\\nthe model to ignore those documents that don’t\\nhelp in answering the question, which we call,\\ndistractor documents. RAFT accomplishes this\\nby citing verbatim the right sequence from the rel-\\nevant document that would help answer the ques-\\ntion. This coupled with RAFT’s chain-of-thought-\\nstyle response helps improve the model’s ability\\nto reason. In domain specific RAG, RAFT consis-\\ntently improves the model’s performance across', metadata={'page': 0, 'source': 'C:\\\\\\\\Users\\\\\\\\ishaan.kohli\\\\\\\\Downloads\\\\\\\\RAFT.pdf'})]}\n"
     ]
    }
   ],
   "source": [
    "input = generate_response(\"What are the benifits of RAFT?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAFT (Reference Attentive Fusion Transformer) is a technique that enables models to learn domain-specific knowledge through fine-tuning while ensuring robustness against inaccurate document retrievals by understanding the dynamics between the question, the retrieved documents, and the appropriate answer. RAFT trains the model to ignore irrelevant or distracting documents and cites verbatim the relevant sequence from the retrieved documents that would help answer the question, coupled with its chain-of-thought-style response to improve the model's ability to reason consistently across domain specific benchmark datasets like RAG.\n"
     ]
    }
   ],
   "source": [
    "print(input['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "om",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
